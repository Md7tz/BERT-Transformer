{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603d728a-4a42-4c64-ad3e-26f7559845f7",
   "metadata": {},
   "source": [
    "# Text classification using BERT\r\n",
    "\r\n",
    "In this notebook, we will utilize a pre-trained deep learning model to analyze some text. The model's output will be used to categorize the text, which is a collection of sentences extracted from movie reviews. Our goal is to determine whether each sentence conveys a positive or negative sentiment towards the subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862aa9fb-4176-4152-ac05-f051c4ca29ec",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "Our objective is to develop a model that can analyze a given sentence and determine whether it expresses a positive sentiment, in which case it should produce a value of 1, or a negative sentiment.\n",
    "\n",
    "The model comprises two components: [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html) and a basic [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) model from scikit-learn.\n",
    "\n",
    "* DistilBERT processes the input sentence and passes on relevant information to the Logistic Regression model for sentiment classification. It is a lighter and faster version of BERT that performs comparably well.\n",
    "\n",
    "* The data shared between the two models is a vector of size 768. This is because DistilBERT represents each input sentence as a sequence of vectors, with each vector having a size of 768. This vector sequence is then fed to the Logistic Regression model for classification.\n",
    "\n",
    "#### Dataset - SST2\n",
    "\n",
    "The SST2 dataset is a widely-used benchmark dataset for sentiment analysis and text classification tasks. It consists of movie reviews from Rotten Tomatoes, with each review labeled as positive or negative. The dataset contains 11,855 training sentences and 2,210 testing sentences, each of which is parsed into a binary parse tree to capture its grammatical structure. The dataset has been used to evaluate the performance of various natural language processing models, including BERT and its variants. You can find the dataset [here](https://nlp.stanford.edu/sentiment/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc132a5-15ad-468d-bdda-7bdc61833a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/20/0a/739426a81f7635b422fbe6cb8d1d99d1235579a6ac8024c13d743efa6847/transformers-4.36.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
      "     ---------------------------------------- 0.0/126.8 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/126.8 kB ? eta -:--:--\n",
      "     -------- ---------------------------- 30.7/126.8 kB 262.6 kB/s eta 0:00:01\n",
      "     ----------- ------------------------- 41.0/126.8 kB 245.8 kB/s eta 0:00:01\n",
      "     -------------------------- ---------- 92.2/126.8 kB 476.3 kB/s eta 0:00:01\n",
      "     ------------------------------------ 126.8/126.8 kB 574.0 kB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.19.3 from https://files.pythonhosted.org/packages/3d/0a/aed3253a9ce63d9c90829b1d36bc44ad966499ff4f5827309099c8c9184b/huggingface_hub-0.20.2-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/42/db/0061fb8004ce9173b9249a0c323c799be51f2c8e6d4ff3cc38b549c3f8b6/tokenizers-0.15.0-cp310-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/1a/ad/a78510af50f569b25dbcbed6e7b94e1d36612ef599eed94ad7e14fb9826c/safetensors-0.4.1-cp310-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp310-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "   ---------------------------------------- 0.0/8.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.2 MB 640.0 kB/s eta 0:00:13\n",
      "    --------------------------------------- 0.1/8.2 MB 1.3 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.3/8.2 MB 2.6 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.3/8.2 MB 2.6 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/8.2 MB 2.4 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.6/8.2 MB 2.5 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.9/8.2 MB 2.8 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.0/8.2 MB 3.0 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.0/8.2 MB 3.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.3/8.2 MB 2.9 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.5/8.2 MB 2.8 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.6/8.2 MB 3.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.6/8.2 MB 3.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.6/8.2 MB 3.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.8/8.2 MB 2.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.0/8.2 MB 2.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.0/8.2 MB 2.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.0/8.2 MB 2.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.0/8.2 MB 2.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.3/8.2 MB 2.5 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.4/8.2 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.7/8.2 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.8/8.2 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.8/8.2 MB 2.5 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/8.2 MB 2.6 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/8.2 MB 2.6 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/8.2 MB 2.6 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.0/8.2 MB 2.6 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.2/8.2 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.3/8.2 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.3/8.2 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.3/8.2 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.5/8.2 MB 2.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 3.6/8.2 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.0/8.2 MB 2.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.2/8.2 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/8.2 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/8.2 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/8.2 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/8.2 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.4/8.2 MB 2.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/8.2 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/8.2 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/8.2 MB 2.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 4.7/8.2 MB 2.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.0/8.2 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.1/8.2 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.1/8.2 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.1/8.2 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.4/8.2 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.4/8.2 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.6/8.2 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 5.7/8.2 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 5.9/8.2 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 6.0/8.2 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.2/8.2 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 6.5/8.2 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.6/8.2 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 6.6/8.2 MB 2.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 6.9/8.2 MB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.1/8.2 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.2/8.2 MB 2.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 7.3/8.2 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.6/8.2 MB 2.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 7.7/8.2 MB 2.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.2 MB 2.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.2/8.2 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.2/8.2 MB 2.5 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "   ---------------------------------------- 0.0/330.3 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 112.6/330.3 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  327.7/330.3 kB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 330.3/330.3 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp310-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 0.0/277.3 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 30.7/277.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 277.3/277.3 kB 2.8 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 9.9 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.4/2.2 MB 8.1 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 5.7 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.9/2.2 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.1/2.2 MB 5.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.2/2.2 MB 4.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.5/2.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.6/2.2 MB 4.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.0/2.2 MB 4.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 4.2 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.16.4\n",
      "    Uninstalling huggingface-hub-0.16.4:\n",
      "      Successfully uninstalled huggingface-hub-0.16.4\n",
      "Successfully installed huggingface-hub-0.20.2 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.36.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "open-interpreter 0.1.4 requires huggingface-hub<0.17.0,>=0.16.4, but you have huggingface-hub 0.20.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers | grep -v \"already\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a1bbc57-4fb0-40a0-8728-3581ec88cdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b6ac8c944454147b29b8f657036fcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee611369-3ebe-444c-bd50-005e34c97af2",
   "metadata": {},
   "source": [
    "### Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31480e6b-51d4-418c-afbf-fddcdb9c727b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  a stirring , funny and finally transporting re...  1\n",
       "1  apparently reassembled from the cutting room f...  0\n",
       "2  they presume their audience wo n't sit still f...  0\n",
       "3  this is a visually stunning rumination on love...  1\n",
       "4  jonathan parker 's bartleby should have been t...  1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv'\n",
    "df = pd.read_csv(url, delimiter='\\t', header=None, nrows=2500)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ec4d5e-5ee9-44f0-965d-4561f440c89a",
   "metadata": {},
   "source": [
    "### Load Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7464489-fa7c-4e3f-a990-c64a72a2a34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f55cb4e10a6498d915a594f16b4aace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Medhat\\Anaconda3\\envs\\torch\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Medhat\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fc66f896b14173969db8ff9d19284c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3058344279db4d1b8b96d2e7b5f4044c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ae6a8e298a40baa9af4a1b48c07dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4cbd903cde4da8b74c5e8951d83a9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DistilBERT\n",
    "model_class, tokenizer_class, pretrained_weights = (transformers.DistilBertModel,\n",
    "                                                   transformers.DistilBertTokenizer,\n",
    "                                                   'distilbert-base-uncased')\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea564cbf-2a28-463b-bc56-813322aee6fd",
   "metadata": {},
   "source": [
    "The code above demonstrates how to load a pre-trained DistilBERT model and tokenizer from the Transformers library by Hugging Face, which can be used for various natural language processing tasks.\r\n",
    "\r\n",
    "First, the `model_class`, `tokenizer_class`, and `pretrained_weights` variables are defined to hold the appropriate classes and weights required for the **DistilBERT** model.\r\n",
    "\r\n",
    "The `DistilBertTokenizer` class is used to tokenize raw text data and prepare it for input to the DistilBERT model. The `DistilBertModel` class is the implementation of the DistilBERT model itself. The `pretrained_weights` variable is set to `distilbert-base-uncased`, which indicates the specific pre-trained DistilBERT model to be used.\r\n",
    "\r\n",
    "Next, the `tokenizer` variable is initialized using the `from_pretrained()` method, which loads the pre-trained tokenizer for the specified DistilBERT model. This allows the raw text data to be tokenized and encoded in a way that can be understood by the model.\r\n",
    "\r\n",
    "Finally, the model variable is initialized using the `from_pretrained()` method, which loads the pre-trained DistilBERT model with the specified weights. This allows the model to be used for various NLP tasks, such as sentiment analysis or text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7572ab1-779c-4d89-9f9a-57b9369b34d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       a stirring , funny and finally transporting re...\n",
       "1       apparently reassembled from the cutting room f...\n",
       "2       they presume their audience wo n't sit still f...\n",
       "3       this is a visually stunning rumination on love...\n",
       "4       jonathan parker 's bartleby should have been t...\n",
       "                              ...                        \n",
       "2495    allegiance to chekhov , which director michael...\n",
       "2496    not only a coming of age story and cautionary ...\n",
       "2497    sparkling , often hilarious romantic jealousy ...\n",
       "2498     a harrowing account of a psychological breakdown\n",
       "2499    a mature , deeply felt fantasy of a director '...\n",
       "Name: 0, Length: 2500, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f156106a-ad8c-4dad-9e8c-a14766d7d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all the reviews in column 0 of the dataframe \"df\"\n",
    "tokenized = df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72245793-cbad-4332-ba71-f6bde7acf81f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [101, 1037, 18385, 1010, 6057, 1998, 2633, 182...\n",
       "1       [101, 4593, 2128, 27241, 23931, 2013, 1996, 62...\n",
       "2       [101, 2027, 3653, 23545, 2037, 4378, 24185, 10...\n",
       "3       [101, 2023, 2003, 1037, 17453, 14726, 19379, 1...\n",
       "4       [101, 5655, 6262, 1005, 1055, 12075, 2571, 376...\n",
       "                              ...                        \n",
       "2495    [101, 14588, 2000, 18178, 25495, 1010, 2029, 2...\n",
       "2496    [101, 2025, 2069, 1037, 2746, 1997, 2287, 2466...\n",
       "2497    [101, 16619, 1010, 2411, 26316, 6298, 14225, 4...\n",
       "2498    [101, 1037, 24560, 2075, 4070, 1997, 1037, 831...\n",
       "2499    [101, 1037, 9677, 1010, 6171, 2371, 5913, 1997...\n",
       "Name: 0, Length: 2500, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12100860-e224-46a8-a3e7-0a010ed780d5",
   "metadata": {},
   "source": [
    "The code above tokenizes a column of reviews in a Pandas DataFrame using the pre-trained tokenizer from the DistilBERT model, which was previously loaded. The resulting tokenized reviews are stored in a new Pandas Series called `tokenized`.\n",
    "\n",
    "First, the `tokenizer.encode()` method is used to encode each review in the DataFrame. The `encode()` method converts the text into a sequence of integers that can be fed into the `DistilBERT` model. The `add_special_tokens=True` argument is passed to add special tokens like **[CLS]** (beginning of sequence) and **[SEP]** (end of sequence) to the beginning and end of each encoded review, respectively.\n",
    "\n",
    "The `apply()` method is used to apply the `tokenizer.encode()` function to each row in the DataFrame column containing the reviews. The resulting tokenized reviews are stored in a new Pandas Series called tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2708f464-edfb-46e3-a345-a0f14b67a90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'stirring',\n",
       " ',',\n",
       " 'funny',\n",
       " 'and',\n",
       " 'finally',\n",
       " 'transporting',\n",
       " 're',\n",
       " 'imagining',\n",
       " 'of',\n",
       " 'beauty',\n",
       " 'and',\n",
       " 'the',\n",
       " 'beast',\n",
       " 'and',\n",
       " '1930s',\n",
       " 'horror',\n",
       " 'films']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de5171d1-3966-42d4-82b5-74414ad99aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualized_sentence_embedding(df: pd.DataFrame, tokenized: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to see tokens and embeddings of the first review in df\n",
    "    \"\"\"\n",
    "    tokens = df.iloc[0,0].split(\" \")\n",
    "    tokens.insert(0, \"CLS\")\n",
    "    tokens.append(\"SEP\")\n",
    "    assert len(tokens) == len(tokenized[0])\n",
    "    token_embeddings = list(zip(tokens, tokenized[0]))\n",
    "    df_token_embeddings = pd.DataFrame(token_embeddings, columns=[\"Tokens\", \"Embeddings\"])\n",
    "    return df_token_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f105a748-c9d2-4374-8ecc-138d5f325fef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CLS</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>1037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stirring</td>\n",
       "      <td>18385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>1010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>funny</td>\n",
       "      <td>6057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>and</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>finally</td>\n",
       "      <td>2633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>transporting</td>\n",
       "      <td>18276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>re</td>\n",
       "      <td>2128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>imagining</td>\n",
       "      <td>16603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>of</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>beauty</td>\n",
       "      <td>5053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>and</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>the</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>beast</td>\n",
       "      <td>6841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>and</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1930s</td>\n",
       "      <td>5687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>horror</td>\n",
       "      <td>5469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>films</td>\n",
       "      <td>3152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>SEP</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Tokens  Embeddings\n",
       "0            CLS         101\n",
       "1              a        1037\n",
       "2       stirring       18385\n",
       "3              ,        1010\n",
       "4          funny        6057\n",
       "5            and        1998\n",
       "6        finally        2633\n",
       "7   transporting       18276\n",
       "8             re        2128\n",
       "9      imagining       16603\n",
       "10            of        1997\n",
       "11        beauty        5053\n",
       "12           and        1998\n",
       "13           the        1996\n",
       "14         beast        6841\n",
       "15           and        1998\n",
       "16         1930s        5687\n",
       "17        horror        5469\n",
       "18         films        3152\n",
       "19           SEP         102"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_token_embeddings = visualized_sentence_embedding(df, tokenized)\n",
    "df_token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d3fe00-a272-4911-b8f4-c5c2559c9f37",
   "metadata": {},
   "source": [
    "### Padding\n",
    "Once the reviews in a DataFrame are tokenized, they are stored as a list of sentences (`tokenized`; data type =`pd.Series`), where each sentence is represented as a list of tokens. In order to process these examples in one batch using BERT, it is necessary to pad all of the lists to the same length. This allows the input to be represented as a single 2-dimensional array, rather than a list of variable-length lists. By doing this, the processing time can be greatly reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "707fa249-c243-4408-a536-2914c0f74c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 65)\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "max_len = max([len(i) for i in tokenized.values])\n",
    "padded_token_embeddings = np.array([i + [0] * (max_len - len(i)) for i in tokenized.values])\n",
    "print(padded_token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4d2c39-5c43-495a-a01d-2122e101f98f",
   "metadata": {},
   "source": [
    "The above code performs the following steps:\r\n",
    "\r\n",
    "1. Initializes `max_len` to zero.\r\n",
    "2. Computes the maximum length of the tokenized reviews using a list comprehension that iterates over the tokenized reviews, returns their lengths. The resulting maximum length is assigned to the `max_len` variable.\r\n",
    "3. Pads the tokenized reviews with zeros to make them all the same length as the maximum length `max_len`. This is done using a list comprehension that iterates over the tokenized reviews, appends 0 to the end of each review until it has the same length as `max_len`, and converts the resulting list of padded reviews to a NumPy array. The resulting padded token embeddings are assigned to the `padded_token_embeddings` variable.\r\n",
    "\r\n",
    "4. Overall, this code computes the maximum length of the tokenized reviews and pads them with zeros to make them all the same length, which is necessary for feeding them into a deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c87f5-93ca-4bf1-a23a-d6e82f12beaa",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "In order to avoid confusing BERT with the padding added to the tokenized reviews, we need to create a separate variable called attention_mask. This variable indicates which tokens should be attended to by the model and which tokens should be ignored (masked) during processing. By setting the attention mask to 1 for the real tokens and 0 for the padding tokens, we can tell BERT to ignore the padding when processing the input. This helps to improve the accuracy of the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9dcc05a3-3984-4113-812e-64d9ff990394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] (2500, 65)\n"
     ]
    }
   ],
   "source": [
    "attention_mask = np.where(padded_token_embeddings != 0, 1, 0)\n",
    "assert attention_mask.shape == padded_token_embeddings.shape\n",
    "print(attention_mask[:2], attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa92374f-dc31-4426-b3b0-d05a204eaa31",
   "metadata": {},
   "source": [
    "### Model inputs\r\n",
    "\r\n",
    "We're now ready to train a deep learning model using PyTorch. We will be using the pre-trained **DistilBERT** model that we previously loaded. First, we need to prepare our inputs for the model. We take our tokenized and padded sentences and convert them into PyTorch tensors using the `torch.tensor()` function.\r\n",
    "\r\n",
    "we can pass the `input_ids` (torch tensor) and `attention_mask` tensors to the DistilBERT model using the `model()` function. The output of the function, `last_hidden_states`, will contain the contextualized embeddings for each token in our input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b9f1f06-6f9d-41c6-9bcf-762bfb993f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.LongTensor(padded_token_embeddings)\n",
    "attention_mask = torch.Tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "afab9e00-0693-4e92-81ba-758dffcafc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting features and labels\n",
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a0654ad4-12e0-48e2-887c-427dd69c6b66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21593435, -0.14028911,  0.00831076, ..., -0.13694832,\n",
       "         0.5867005 ,  0.20112693],\n",
       "       [-0.17262718, -0.14476153,  0.00223438, ..., -0.1744257 ,\n",
       "         0.21386446,  0.37197465],\n",
       "       [-0.05063373,  0.07203954, -0.02959727, ..., -0.0714895 ,\n",
       "         0.7185238 ,  0.2622547 ],\n",
       "       ...,\n",
       "       [-0.03103156,  0.06106165, -0.0742958 , ..., -0.12653553,\n",
       "         0.55033255,  0.45576078],\n",
       "       [-0.37851074, -0.04516774, -0.18900727, ..., -0.14968894,\n",
       "         0.2870552 ,  0.2913559 ],\n",
       "       [-0.27609336, -0.02547334, -0.1110792 , ..., -0.26346034,\n",
       "         0.5565323 ,  0.42118883]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fcdb33-c378-4dca-8c36-4b51454309a7",
   "metadata": {},
   "source": [
    "**Explanation for feature extraction from `last_hidden_states`:**\r\n",
    "\r\n",
    "Suppose we have a batch of 2500 input sentences, where each sentence is tokenized and padded to a length of 65. So, the shape of our padded array would be (2500, 65).\r\n",
    "\r\n",
    "Now, we pass this padded array to BERT using the `model()` function, and it returns a tensor `last_hidden_states` of shape (2500, 65, 768). Here, 2500 is the batch size, 65 is the length of the padded sentence, and 768 is the size of the BERT embedding for each token.\r\n",
    "\r\n",
    "To get a fixed-length representation of each sentence, we take the first token of each sentence, which is the `[CLS]` token. So, we extract the embeddings corresponding to the `[CLS]` token, which is located at index 0 in the second dimension of last_hidden_states.\r\n",
    "\r\n",
    "To get these embeddings for each sentence in the batch, we use the slicing operation `[:,0,:]`. This selects all elements along the first dimension (which corresponds to the batch size), the first element along the second dimension (which corresponds to the `[CLS]` token), and all elements along the third dimension (which corresponds to the embedding size). This returns a tensor of shape (2500, 768), where each row corresponds to the embedding of a single sentence.\r\n",
    "\r\n",
    "Finally, we convert this tensor to a numpy array using `.numpy()`, which gives us a 2D numpy array features of shape (2500, 768), where each row represents the fixed-length representation of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2e3cd598-a47a-4d0f-ba23-99cea11f11e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       0\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "2495    0\n",
       "2496    1\n",
       "2497    1\n",
       "2498    1\n",
       "2499    1\n",
       "Name: 1, Length: 2500, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df[1]\n",
    "assert len(features) == len(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6edb5a3-12bb-44e3-aa40-e3c3e9ee34a6",
   "metadata": {},
   "source": [
    "### Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5f579ffd-3928-4df5-96da-4e4a7286980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d7d0df-e377-40e1-8417-f1baa4b55384",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7347b854-b1ce-488e-9844-267c1273cd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=5, max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=5, max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(C=5, max_iter=1000)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression(C=5, max_iter=1000)\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e96f40d-b818-40c8-90e5-8ae2610e11cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.848"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how our trained LR model performs on the test set\n",
    "lr_clf.score(test_features, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
